namespace pelican {

/**
\page user_dataOutput Data Output

\section user_dataOutput_introduction Introduction

The following sections of the reference documentation are relevant
for exporting data from Pelican:

\li \link user_referenceDataBlobs Data Blob reference\endlink
\li \link user_referenceOutputStreamers Output streamer reference\endlink


\section user_dataOuput Data Output from a Pipeline

In simple cases, you just call the AbstractPipeline::dataOutput() method, and
supply a pointer to the data blob that must be sent:

@code
MyBlob* data;
dataOutput(data);
@endcode

This will push data onto an output stream named after the data type
("MyBlob" in this case). The AbstractPipeline::dataOutput() method can take
a string as a second argument to assign data to a stream with an alternative
name. This can be useful where there you are exporting multiple streams of the
same data type.

\section user_dataOuput_blobs Data-Blob Output Requirements

It is important to note that any DataBlob destined for output should
reimplement the DataBlob::serialise() and DataBlob::deserialise() methods.
The serialise routine should write the complete contents of the data blob to 
the output device provided in the arument. The deserialise method should do the reverse,
reading the complete contents of the blob from the input device. Note that the byte
order of the machine that the serialise() method was called is also provided
to help you interpret binary streams if necessary.

\subsection user_dataOuput_tutorialBlobs Tutorial Data Blob Output

The SignalData::serialise() and SignalData::deserialise() methods for our
SignalData data-blob are shown in the C++ source file, below:

\include SignalData.cpp

We use a QDataStream to be endian-neutral, so we don't actually need use the
Endian type argument of the deserialise routine: this provides the endianness
of the system that serialised the blob.

\section user_dataOuput_outputStreamManager The Output Stream Manager

The actual destination of the data is controlled by the OutputStreamManager
object. This can be configured either through the configuration file, or you
can manipulate the object directly.

Pelican comes with two output streamer modules by defualt
(described in more detail in the reference manual).
Both assume that
you have implemented the serialise() and deserialise methods.
However, this may be insufficient for your needs if you have a custom file
format, or a specific database to fill.
In these cases you will need to write your own output streamer, which we will
give an example of after discussing those provided by Pelican. 

@subsection user_dataOutput_DataBlobFile The DataBlobFile
This will store a binary format file on the local machine.
There is a \em DataBlobFileReader available for reading
this format and extracting its contents as DataBlobs for further processing 
as required.

@subsection user_dataOutput_TCPBlobServer The PelicanTCPBlobServer
The PelicanTCPBlobServer allows you to stream data to one or more other machines
using a network TCP connection.
As any processing done in an output streamer is done serially within the pipeline
processing, and so any latencies, e.g disk writes, will slow down your processing.
If this will be a problem, it is better to export the data via the 
PelicanTCPBlobServer and process the streams elsewhere.

@subsection user_dataOutput_example  Tutorial Output

We take the example pipeline from previous sections to illustrate how the data
output system works. We will output the SignalData data-blob before and
after processing to different streams, called "pre" and "post". We will then
connect different modules to these streams to show how to redirect the data.
The first thing to do is to instruct the pipeline to output data at the
required points by calling AbstractPipeline::dataOutput(), as shown.
Note that we must specify unique stream names, since both outputs use a
data-blob of type SignalData.

@code
void SignalProcessingPipeline::run(QHash<QString, DataBlob*>& data)
{
    // Get pointers to the remote data blob(s) from the supplied hash.
    SignalData* inputData = (SignalData*) remoteData["SignalData"];

    // Output the input data.
    dataOutput(inputData, "pre");

    // Run each module as required.
    // ...

    // Output the processed data.
    dataOutput(outputData, "post");
}
@endcode

@subsubsection user_dataOutput_xmlconfig Configuring the Output Stream Manager

So now we have two data streams which we can direct wherever required using the
output manager:

\li The unprocessed data (the "pre" stream) will be directed to a
    comma-separated value (CSV) file on the local machine using our own custom
    output streamer.
\li Similarly, we direct the processed data stream ("post") to a different
    CSV file using another instance of our own output streamer.
\li We also save a binary format file of the "post" data stream using
    the default DataBlobFile
\li Finally, we will also direct both streams to the standard
    PelicanTCPBlobServer for export to other clients.

The local CSV files will be written using our custom OutputStreamerExample
object described in the
\ref user_referenceOutputStreamers "Output Streamer Reference." Please see that
page for example code that describes how to implement the required output
streamer.

The \c output section of our configuration file will look like this:
\verbatim
<output>
    <streamers>
        <PelicanTCPBlobServer active="true">
             <connection port="1234" />
        </PelicanTCPBlobServer>
        <DataBlobFile>
            <file name="datablob.dblob" type="heterogeneous">
        </DataBlobFile>
        <OutputStreamExample name="precsv" active="true">
             <file name="pre.csv" />
        </OutputStreamExample>
        <OutputStreamExample name="postcsv" active="true">
             <file name="post.csv" />
        </OutputStreamExample>
    </streamers>
    <dataStreams>
        <stream name="all" listeners="PelicanTCPBlobServer" />
        <stream name="post" listeners="DataBlobFile" />
        <stream name="post" listeners="postcsv" />
        <stream name="pre" listeners="precsv" />
    </dataStreams>
</output>
\endverbatim

The \c streamers section describes the configuration for each output streamer.
Its presence in the XML will cause the OutputStreamManager to attempt to
instantiate an object of that type (unless the \c active attribute is set to
false). Where there are more than two objects of the same type, you must
provide a \c name attribute to disambiguate them.

The \c dataStreams section is used to map the streams to these objects.
The \c listeners attribute is a comma separated list of the names of the
streamers that should be associated with the named stream.
Note the special stream name "all" which will cause all streams to be piped
to the listed listeners.

\section dataOutput_client Reading from the PelicanTCPBlobServer

By piping the streams to the PelicanTCPBlobServer, we have the ability to
connect from elsewhere to read any of the data streams we are interested in.
Pelican provides a client (the DataBlobClient object) to connect to this server
and subscribe to any number of streams, reconstructing the DataBlob
objects as they are passed over the TCP stream. Use of the DataBlobClient is
outside the scope of this tutorial, but it is described in the Pelican API
reference.

The final section of the tutorial shows how all the components are linked
together to build a working Pelican system.

\latexonly
\clearpage
\endlatexonly

*/

}
